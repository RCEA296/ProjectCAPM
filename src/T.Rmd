---
title: "Exploratory Data Analysis of Tsunami Events from Seismic Data"
author: "Angel Ortiz, Rayane Cascon"
date: "2025-03-11"
output:
  html_document:
    css: mytheme.css
    theme: cerulean
    highlight: tango
    number_sections: false
    toc: true
    toc_depth: 1
editor_options:
  chunk_output_type: console
---



```{r global_options, include=T, echo = F}
knitr::opts_chunk$set(echo = T, warning=FALSE, message=FALSE)
```

# Exploratory Data Analysis of Tsunami Events from Seismic Data
The aim of this study is to analyse variables related with Seismic events 
to create a proper dataset for tsunami prediction. 

**Project:** Tsunami Prediction Analysis from Earthquake Parameters  
**Data Source:** https://www.kaggle.com/datasets/warcoder/earthquake-dataset 
**License:** Creative Commons Attribution 4.0 International (CC BY 4.0)  
[https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)
***
<br>

# 1. Setup and Data Loading

## 1.1 Package Installation and Loading

```{r, message=FALSE}
# Install required packages if not already installed
required_packages <- c(
  "tidyverse",  
  "dplyr",      
  "ggplot2",    
  "factoextra", 
  "GGally",     
  "tidyr",      
  "maps",      
  "rpart",    
  "cluster",    
  "mclust",
  "reshape2"
)

# Check and install packages
for (pkg in required_packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg, dependencies = TRUE)
  }
}
```

## 1.2 Load Required Libraries and Prepare Environment

```{r}
library(tidyverse)
library(tidyr)
library(dplyr)
library(ggplot2)
library(factoextra)
library(GGally)
library(maps)
library(rpart)
library(cluster)
library(mclust)
library(reshape2)

rm(list = ls())

```

## 1.3 Data Import
```{r}

knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
tsunami_data = read.csv("data/TsunamiData.csv", header= TRUE)
nrow(tsunami_data)
str(tsunami_data)
head(tsunami_data)
```

The dataset has been successfully loaded.
The structure of the data can be seen above.

***

<br>

# 2. Data Preprocessing

## 2.1 Initial Data Exploration

```{r}
cat("Dataset dimensions:", dim(tsunami_data), "\n")
cat("Column names:", paste(names(tsunami_data), collapse = ", "), "\n\n")

# Examine target variable distribution
tsunami_data %>% dplyr::count(tsunami)
```

The dataset contains `r nrow(tsunami_data)` observations and `r ncol(tsunami_data)` variables.
For the purpose of the analysis, the variable "tsunami" will be considered as the 
target variable.

***

## 2.2 Feature Selection and Removal

```{r}
# Remove variables without useful information for prediction
tsunami_data %>% dplyr::count(alert)
tsunami_data <- tsunami_data %>% 
  select(-c("title", "continent", "location", "country", "net", "alert"))
```

Six variables that were not suitable for predictive modeling were removed:
- **title**: Free text description (not quantifiable)
- **continent, location, country**: Too granular for identifying global patterns
- **net**: Network identifier (administrative data, not predictive)
- **alert**: Too much Na's values

*** 

## 2.3 Magnitude Type Filtering

```{r}

tsunami_data %>% dplyr::count(magType)

tsunami_data <- tsunami_data %>% 
  filter(magType %in% c("mw", "mwb", "mwc", "mww")) %>% 
  select(-magType)
```

The data was filtered to include only moment magnitude (Mw) measurements since
it is the modern standard for measuring earthquake size and it minimizes
the bias that arises from using other scales.

After filtering, `r nrow(tsunami_data)` observations remained, 
with no significant loss of data.

***

## 2.4 Convert Year variable 

```{r}

tsunami_data <- tsunami_data %>% 
  mutate(year = as.numeric(substr(date_time, 7, 11))) %>% 
  select(-date_time)

```

The "year" variable is extracted from datetime. Numeric type of variable suits
 study. This allows us to investigate whether tsunami patterns have changed over time.
 
***

## 2.5 Missing values

```{r}
  tsunami_data %>%
  summarise(across(everything(), ~sum(is.na(.))))
```

---

<br>

# 3. Feature Engineering and Transformation

## 3.1 Outlier Detection

```{r}

tsunami_data %>% 
  select(sig, nst, dmin, gap, depth) %>% 
  stack() %>% 
  ggplot(aes(y = values)) +
  geom_boxplot(outlier.color = "red", fill = "lightblue") +
  facet_wrap(~ ind, scales = "free") + 
  theme_bw()
```

The boxplots reveal that many values are concentrated at zero,
which represents the absence of tsunami-inducing factors rather than true outliers. 
While these zero-inflated distributions are unusual, they are meaningful in this context and should not be removed.

***

## 3.2 Distribution Analysis

```{r}

tsunami_data %>% 
  select(sig, nst, dmin, gap, depth) %>% 
  stack() %>% 
  ggplot(aes(x = values)) +
  geom_histogram(bins = 30, fill = "lightblue", color = "black") +
  facet_wrap(~ ind, scales = "free") +
  labs(
    title = "Distribution of Seismic Parameters (Raw Scale)",
    x = "Value", 
    y = "Frequency"
  ) +
  theme_bw()
```

The raw distributions are highly right-skewed, suggesting that a logarithmic transformation would be appropriate. 
This is consistent with how earthquake magnitude is measured on the Richter scale (a logarithmic scale).

***

## 3.3 Logarithmic Transformation

```{r}
# Apply log10 transformation to right-skewed variables
tsunami_data <- tsunami_data %>%
  mutate(across(c(sig, nst, gap, dmin, depth),
                ~ log10(.x + 1)))

# Visualize transformed distributions
tsunami_data %>% 
  select(sig, nst, dmin, gap, depth) %>% 
  stack() %>% 
  ggplot(aes(x = values)) +
  geom_histogram(bins = 30, fill = "lightblue", color = "black") +
  facet_wrap(~ ind, scales = "free") +
  labs(
    title = "Distribution of Seismic Parameters (Log-Transformed)",
    x = "Log10(Value + 1)", 
    y = "Frequency"
  ) +
  theme_bw()
```

After applying the log transformation (adding 1 to avoid log(0)), 
the distributions are much more symmetric and approximately normal.

***

## 3.4 Data Normalization

```{r}

normalized_tsunami_data <- tsunami_data %>%
  mutate(across(-tsunami, ~ scale(.x)))
```
Final step before applying any model.

---

<br>

# 4. Data Visualization

## 4.1 Correlation Analysis

```{r}

ggcorr(normalized_tsunami_data, label = TRUE) + 
  labs(title = "Correlation Matrix of Seismic Parameters")
```

The correlation matrix reveals the relationships between variables.
Most correlations are moderate, 
suggesting that the variables provide somewhat independent information for predicting tsunami occurrence.

***

## 4.2 Geospatial Visualization

```{r}

world <- map_data("world")

tsunami_data %>% 
  select(latitude, longitude, tsunami) %>% 
  ggplot() +
  geom_polygon(
    data = world,
    aes(x = long, y = lat, group = group),
    fill = "gray90", 
    color = "white"
  ) +
  geom_point(
    aes(x = longitude, y = latitude, color = factor(tsunami)),
    size = 2, 
    alpha = 0.6
  ) +
  scale_color_manual(
    values = c("0" = "blue", "1" = "red"),
    labels = c("No Tsunami", "Tsunami"),
    name = "Event Type"
  ) +
  coord_fixed(1.3) +
  labs(
    title = "Global Distribution of Earthquake-Tsunami Events", 
    subtitle = "Concentrated along tectonic plate boundaries",
    x = "Longitude", 
    y = "Latitude"
  ) +
  theme_bw()
```

The plot clearly shows that earthquakes (and especially tsunamis) are 
concentrated along tectonic plate boundaries, particularly the Pacific Ring of Fire.
The relationship between location and tsunami occurrence is non-linear and spatially
clustered, which cannot be captured by simple linear regression.

***

---

<br>

# 5. Predictive Modeling

## 5.1 Linear Regression Analysis

```{r}

multi_tsunami <- lm(
  tsunami ~ cdi + mmi + sig + nst + dmin + gap + depth + latitude + longitude + year, 
  data = tsunami_data
)

summary(multi_tsunami)
```

```{r linear-pred, fig.cap="Linear regression predictions vs year"}

pred <- predict(multi_tsunami)
qplot(
  tsunami_data$year, 
  pred, 
  main = "Linear Regression: Year vs Predicted Tsunami Probability",
  xlab = "Year",
  ylab = "Predicted Value"
) +
  theme_bw()
```

**Linear Regression Conclusion:** 
While linear regression provides initial insights into variable relationships, it is fundamentally inappropriate for binary outcomes.
The year variable shows the clearest linear relationship, but the model produces predicted probabilities outside the valid [0,1] range. 
This indicates that logistic regression is necessary for proper probabilistic predictions.

***

## 5.2 Logistic Regression Model

```{r}

logistic_tsunami <- tsunami_data %>% 
  glm(
    tsunami ~ cdi + mmi + sig + nst + dmin + gap + depth + latitude + longitude + year,
    data = ., 
    family = binomial
  )

revised_logistic_tsunami <- tsunami_data %>% 
  glm(
    tsunami ~ cdi + nst + dmin + year, 
    data = ., 
    family = binomial
  )

summary(revised_logistic_tsunami)
```

The logistic regression model confirms that the most significant predictors of tsunami occurrence are:
- **Year** (positive effect): Recent earthquakes are more likely to be associated with tsunamis
- **CDI** (negative effect): Higher perceived intensity indicates land-based earthquakes
- **NST** (negative effect): More detecting stations indicate proximity to land
- **DMIN** (negative effect): Minimum distance to stations shows counterintuitive negative relationship

## 5.3 Model Evaluation Metrics

```{r}
# Calculate McFadden's Pseudo R-squared
rdev <- summary(revised_logistic_tsunami)$deviance
nulldev <- summary(revised_logistic_tsunami)$null.deviance
mcfadden_r2 <- 1 - (rdev / nulldev)

# Mean Squared Error
predicted_probs <- predict(revised_logistic_tsunami, type = "response")
actual <- tsunami_data$tsunami
mse <- mean((actual - predicted_probs)^2)

# Log-Loss
predicted_probs <- pmin(pmax(predicted_probs, 1e-15), 1 - 1e-15)
log_loss <- -mean(actual * log(predicted_probs) + (1 - actual) * log(1 - predicted_probs))

```

**Model Performance Analysis:**

McFadden's Pseudo RÂ² = `r round(mcfadden_r2, 4)`: 
This value indicates excellent model fit. For binary logistic regression, values greater than 0.2 are considered excellent,
and values greater than 0.4 are exceptional. Our model explains approximately `r round(mcfadden_r2 * 100, 1)`% 
of the deviance in tsunami probability, which is outstanding for a binary outcome model.

Mean Squared Error = `r round(mse, 4)`: This indicates good predictive accuracy on the probability scale.

Log-Loss = `r round(log_loss, 4)`: 
This is the appropriate loss function for logistic models. 
Lower values indicate better probabilistic predictions. Perfect predictions yield log-loss = 0.

***

## 5.4 Odds Ratios Interpretation

```{r}
revised_logistic_summary <- summary(revised_logistic_tsunami)
multiplicative_effects <- exp(revised_logistic_summary$coefficients[-1, "Estimate"])
print(round(multiplicative_effects, 4))
```

**Interpretation of Predictor Effects:**

- **CDI** (Odds Ratio = `r round(multiplicative_effects["cdi"], 4)`): For each unit increase in Community Determined Intensity, the odds of a tsunami decrease by approximately `r round((1 - multiplicative_effects["cdi"]) * 100, 1)`%. This suggests that earthquakes felt intensely on land (higher CDI) are less likely to cause tsunamis because they occurred near populated areas rather than offshore.

- **NST** (Odds Ratio = `r round(multiplicative_effects["nst"], 4)`): For each additional seismic station detecting the event, the odds of a tsunami are multiplied by `r round(multiplicative_effects["nst"], 3)`. More stations detecting an event indicates the earthquake occurred near stations (typically on land), making tsunami generation less likely.

- **DMIN** (Odds Ratio = `r round(multiplicative_effects["dmin"], 4)`): Greater minimum distance to seismic stations shows a counterintuitive negative relationship with tsunami probability. This may reflect station placement bias, where stations are concentrated in populated (land-based) areas.

- **Year** (Odds Ratio = `r round(multiplicative_effects["year"], 4)`): More recent years show higher tsunami odds. This could reflect either improved detection capabilities over time or genuine changes in tectonic activity patterns.

## 5.5 Cross-Validation

```{r}
# Set up 10-fold cross-validation with decision trees
k <- 10
kfolds <- sample(rep(1:k, length.out = nrow(tsunami_data)))

comb <- expand.grid(
  maxdepth = c(seq(1, 5, 1)),
  mins = c(seq(2, 40, 5)),
  cp = c(seq(0.01, 1, 0.1))
)

acc <- c()
logloss <- c()

for (i in 1:nrow(comb)) {
  kacc <- c()
  klog <- c()
  
  for (j in 1:k) {
    train <- tsunami_data[kfolds != j, ]
    test <- tsunami_data[kfolds == j, ]
    
    model <- rpart(
      tsunami ~ ., 
      data = train, 
      method = "class",
      control = rpart.control(
        cp = comb$cp[i],
        minsplit = comb$mins[i],
        maxdepth = comb$maxdepth[i]
      )
    )
    
    pred <- predict(model, newdata = test, type = "class")
    kacc[j] <- mean(pred == test$tsunami)
    
    pred_prob <- predict(model, newdata = test, type = "prob")[, 2]
    pred_prob <- pmax(pmin(pred_prob, 1 - 1e-15), 1e-15)
    
    y <- test$tsunami
    klog[j] <- -mean(y * log(pred_prob) + (1 - y) * log(1 - pred_prob))
  }
  
  acc[i] <- mean(kacc)
  logloss[i] <- mean(klog)
}

best_acc <- acc[which.max(acc)]
best_logloss <- logloss[which.min(logloss)]

```

**Cross-Validation Results:**

The model achieves an accuracy of `r round(best_acc, 4)` (`r round(best_acc * 100, 2)`%) across all folds with a log-loss of `r round(best_logloss, 4)`. These results confirm that our model has strong predictive power and is not overfitting to the training data.
The consistency across folds validates that the model generalizes well to unseen data.

---

<br>

# 6. Principal Component Analysis (PCA)

## 6.1 PCA Computation and Variance

```{r}
# PCA on standardized data (excluding target variable)
pca <- prcomp(normalized_tsunami_data %>% select(-tsunami), scale. = TRUE)

summary(pca)
```

```{r}

fviz_screeplot(pca, addlabels = TRUE) +
  labs(
    title = "Scree Plot: Variance Explained by Principal Components",
    subtitle = "First 5 PCs capture >80% of total variance",
    x = "Principal Component",
    y = "Percentage of Explained Variance"
  )

var_explained <- (pca$sdev^2 / sum(pca$sdev^2)) * 100
cumvar_explained <- cumsum(var_explained)
```

The scree plot shows that the first five principal components explain `r round(cumvar_explained[5], 2)`% 
of the total variance in the data. 
This substantial proportion of explained variance suggests that dimensionality 
reduction is possible without losing critical information.

***

## 6.2 Component Interpretation

```{r}
# Variables contributing most to PC1
barplot(
  pca$rotation[, 1], 
  las = 2, 
  col = "darkblue",
  main = "Variable Contributions to PC1",
  ylab = "Loading"
)
```

```{r}
fviz_contrib(pca, choice = "var", axes = 1) +
  labs(title = "Variable Contributions (%) to Principal Component 1")
```

PC1 primarily captures variation in:
- **Year** (temporal changes)
- **Sig** (signal magnitude)
- **NST** (number of detecting stations)
- **Gap** (azimuthal gap between stations)

These contributions suggest that PC1 represents the overall seismic 
activity level and detection quality of earthquakes in the dataset.

***

## 6.3 PCA Visualizations

```{r}

fviz_pca_var(
  pca, 
  col.var = "contrib",
  gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
  repel = TRUE
) +
  labs(title = "PCA: Variable Contributions and Correlations")
```

```{r}

fviz_pca_biplot(
  pca, 
  repel = TRUE,
  col.ind = as.factor(tsunami_data$tsunami),
  palette = c("blue", "red"),
  addEllipses = TRUE,
  label = "var"
) +
  labs(
    title = "PCA Biplot: Observations and Variable Loadings",
    subtitle = "Points colored by tsunami occurrence"
  )
```

**PCA Conclusions:**

1. **Dimensionality Reduction:** The first 5 principal components capture over 80% of the total variance, enabling effective data reduction while retaining most information.

2. **Underlying Structure:** PCA reveals that seismic parameters naturally cluster into groups representing detection quality, magnitude measures, and spatial/temporal information.

3. **Interpretation Challenge:** While PCA successfully reduces dimensions, the principal components are complex linear combinations of original variables, making direct interpretation challenging.

4. **Practical Application:** Original variables or factor analysis may provide more interpretable insights into the seismic processes driving tsunami occurrence.

***
---

<br>

# 7. Factor Analysis

## 7.1 Factor Analysis without Rotation
```{r}
factor_analysis_none <- factanal(
tsunami_data %>% select(-tsunami),
factors = 3,
rotation = "none"
)

print(factor_analysis_none)

```


The unrotated factor analysis provides a baseline, but the loadings are difficult to interpret because variables l
oad on multiple factors simultaneously.

***

## 7.2 Varimax Rotation (Orthogonal)

```{r}

factor_analysis_varimax <- factanal(
  tsunami_data %>% select(-tsunami),
  factors = 3,
  rotation = "varimax"
)

print(factor_analysis_varimax)

# Extract loadings
loadings_varimax <- factor_analysis_varimax$loadings

# Visualize loadings
loadings_df <- as.data.frame(unclass(loadings_varimax))
loadings_df$Variable <- rownames(loadings_df)  # Changed to capital V for clarity
loadings_long <- melt(loadings_df, id.vars = "Variable", variable.name = "Factor", value.name = "Loading")

ggplot(loadings_long, aes(x = Variable, y = Loading, fill = Variable)) +
  geom_col() +
  facet_wrap(~ Factor, ncol = 1) +  # Changed from variable.1 to Factor
  coord_flip() +
  theme_bw() +
  labs(
    title = "Factor Loadings (Varimax Rotation)",
    subtitle = "Orthogonal rotation maximizes variance of squared loadings",
    x = "Variable",
    y = "Loading"
  ) +
  guides(fill = "none")

```

## 7.3 Promax Rotation (Oblique)

```{r}
factor_analysis_promax <- factanal(
  tsunami_data %>% select(-tsunami),
  factors = 3,
  rotation = "promax"
)

print(factor_analysis_promax)

loadings_promax <- factor_analysis_promax$loadings

loadings_df_promax <- as.data.frame(unclass(loadings_promax))
loadings_df_promax$Variable <- rownames(loadings_df_promax)  # Changed to capital V
loadings_long_promax <- melt(loadings_df_promax, id.vars = "Variable", variable.name = "Factor", value.name = "Loading")

ggplot(loadings_long_promax, aes(x = Variable, y = Loading, fill = Variable)) +
  geom_col() +
  facet_wrap(~ Factor, ncol = 1) +  # Changed from variable.1 to Factor
  coord_flip() +
  theme_bw() +
  labs(
    title = "Factor Loadings (Promax Rotation)",
    subtitle = "Oblique rotation allows factors to correlate",
    x = "Variable",
    y = "Loading"
  ) +
  guides(fill = "none")
```

**Factor Analysis Conclusions:**

Factor analysis identifies three underlying latent factors that explain the covariance structure of the seismic parameters. Unlike PCA which maximizes variance explained, factor analysis focuses on shared variance among variables, separating it from unique variance.

The analysis reveals three underlying dimensions of seismic events:

1. **Detection Quality Factor:** Primarily loads on detection-related variables (NST, gap, dmin), representing monitoring network characteristics. High values indicate earthquakes detected by many well-distributed seismic stations.

2. **Earthquake Intensity Factor:** Loads heavily on magnitude-related variables (magnitude, sig, mmi, cdi), representing the strength and impact of seismic events. This captures the raw energy and perceived impact of earthquakes.

3. **Spatiotemporal Context Factor:** Loads on spatial-temporal variables (latitude, longitude, year, depth), reflecting geographic and temporal patterns of seismic activity. This dimension captures where and when earthquakes occur.

Between the two rotation methods, varimax rotation produces orthogonal (uncorrelated) factors with simpler interpretation, while promax rotation allows factors to correlate, which is more realistic for seismic data where detection quality, intensity, and location are naturally interrelated. Both rotation methods improve interpretability compared to the unrotated solution by producing clearer factor structures where each factor is dominated by a subset of variables.

---

<br>

# 8. Clustering Analysis

## 8.1 Data Preparation

```{r}

cluster_normalized <- normalized_tsunami_data %>% select(-tsunami)
cluster_original <- tsunami_data %>% select(-tsunami)
```

## 8.2 Elbow Method for Optimal Clusters

```{r elbow-method, fig.cap="Elbow method showing optimal k=4"}

ks <- seq(2, 10, 1)
tot_withinss <- ks

for (i in 1:length(ks)) {
  cluster_scaled <- kmeans(cluster_normalized, ks[i], nstart = 10)
  tot_withinss[i] <- cluster_scaled$tot.withinss
}

# Plot elbow curve
elbow <- data.frame(ncluster = ks, Intravariance = tot_withinss)
ggplot(data = elbow) +
  aes(x = ncluster, y = Intravariance) +
  geom_line() +
  geom_point(aes(x = ks[3], y = tot_withinss[3]), size = 3, color = 'blue') +
  geom_vline(xintercept = 4, linetype = "dashed", color = "red") +
  labs(
    title = "Elbow Method for Optimal Number of Clusters",
    subtitle = "Clear elbow at k=4",
    x = "Number of Clusters",
    y = "Within-Cluster Sum of Squares"
  ) +
  theme_bw()
```

The elbow plot clearly shows an inflection point at k=4 clusters, 
indicating that this is the optimal number of clusters. Beyond k=4, 
the reduction in within-cluster variance becomes marginal, suggesting no
benefit to additional clusters.

## 8.3 K-Means Clustering with Optimal k=4

```{r}

k <- 4
fit_kmeans <- kmeans(cluster_normalized, centers = k, nstart = 1000)
groups <- fit_kmeans$cluster


barplot(
  table(groups), 
  col = "darkblue",
  main = "Cluster Size Distribution (k=4)",
  xlab = "Cluster",
  ylab = "Number of Observations"
)

# Calculate mean tsunami probability by cluster
tsunami_by_cluster <- tapply(normalized_tsunami_data$tsunami, groups, mean)

print(round(tsunami_by_cluster, 4))
```

The clusters are reasonably well-distributed. Computing the mean tsunami probability reveals:

```{r}
cluster_risk <- data.frame(
  Cluster = 1:k,
  Size = as.numeric(table(groups)),
  Tsunami_Probability = round(tsunami_by_cluster, 4),
  Risk_Level = ifelse(tsunami_by_cluster > 0.3, "High", "Low")
)
print(cluster_risk)
```

Two clusters (typically 1 and 2) show high tsunami probability (> 40%), while two clusters show low probability (< 10%).

```{r}
centers <- fit_kmeans$centers
tidy <- cbind(
  gather(as.data.frame(t(centers)), "cluster", "coor"),
  var = rep(colnames(centers), k),
  size = rep(table(fit_kmeans$cluster), each = ncol(centers))
)

tidy %>%
  ggplot(aes(x = var, y = coor, fill = var)) +
  geom_col() +
  facet_wrap(~ cluster) +
  theme(axis.text.x = element_blank()) +
  labs(
    title = "Cluster Profiles (k=4)",
    subtitle = "Standardized variable means within each cluster",
    x = "Variable",
    y = "Standardized Value"
  )
```

```{r}
# 2D visualization in PCA space
fviz_cluster(
  fit_kmeans,
  data = cluster_normalized,
  labelsize = 0,
  stand = FALSE
) +
  labs(
    title = "K-Means Clusters Visualized in PCA Space",
    subtitle = "k=4, normalized data"
  )
```

## 8.4 Silhouette Analysis

```{r}
# Calculate silhouette coefficients
d <- dist(cluster_normalized, method = "euclidean")
sil <- silhouette(groups, d)

plot(
  sil, 
  col = 1:k, 
  main = "Silhouette Plot for K-Means Clustering (k=4)",
  border = NA
)

sil_avg_width <- mean(sil[, "sil_width"])
cat("Average silhouette width:", round(sil_avg_width, 4), "\n")
```

**Silhouette Analysis Interpretation:**

The average silhouette width of `r round(sil_avg_width, 4)` indicates reasonable cluster quality. The silhouette coefficient ranges from -1 to 1, where:
- Values > 0.5 indicate good clustering
- Values near 0 indicate borderline assignments
- Negative values suggest potential misclassification

Our value of `r round(sil_avg_width, 4)` represents a solid clustering solution, showing that most observations are reasonably well-assigned to their clusters.

## 8.5 Partitioning Around Medoids (PAM)

```{r}
# PAM clustering as a robust alternative to K-Means
fit_pam <- eclust(
  cluster_normalized, 
  FUNcluster = "pam", 
  stand = TRUE, 
  k = 4,
  graph = FALSE, 
  nstart = 1000
)

fviz_cluster(
  fit_pam, 
  geom = c("point"),
  main = "PAM Clustering with Medoids (k=4)",
  ellipse.type = "norm",
  labelsize = 3,
  repel = TRUE
)
```

**PAM Clustering Advantages:**

PAM (Partitioning Around Medoids) is a more robust clustering method than K-Means because:
- It uses actual data points as cluster centers rather than artificial centroids
- It is less sensitive to outliers and extreme values
- The medoid centers are interpretable as representative earthquakes
- It can work with any distance metric

## 8.6 Hierarchical Clustering (Validation)

```{r}

hc <- hclust(dist(cluster_normalized), method = "ward.D2")

plot(hc, 
     main = "Hierarchical Clustering Dendrogram",
     xlab = "Observation", 
     ylab = "Distance")

# Calculate cut height for k=4
heights <- sort(hc$height, decreasing = TRUE)
cut_height <- heights[4]

abline(h = cut_height, col = "red", lwd = 2)

hc_clusters <- cutree(hc, k = 4)
```

```{r}

hc_silhouette <- silhouette(hc_clusters, d)

plot(hc_silhouette, 
     col = 1:4, 
     main = "Silhouette Plot for Hierarchical Clustering (k=4)",
     border = NA)

hc_avg_width <- mean(hc_silhouette[, "sil_width"])
```

```{r}

for (i in 1:k) {
  
  cat("\n=== CLUSTER", i, "===\n")
  cat("Size:", sum(groups == i), "observations\n")
  cat("Tsunami probability:", round(tsunami_by_cluster[i], 4), "\n")
  
  # Key features
  cluster_profile <- fit_kmeans$centers[i, ]
  top_positive <- names(sort(cluster_profile, decreasing = TRUE)[1:3])
  top_negative <- names(sort(cluster_profile, decreasing = FALSE)[1:3])
  cat("Highest values:", paste(top_positive, collapse = ", "), "\n")
  cat("Lowest values:", paste(top_negative, collapse = ", "), "\n")
}
```

**Cluster Interpretation for Tsunami Risk:**

Clusters 1 and 2 represent **high-risk** events characterized by:
- Recent years (improved detection or genuine increase in activity)
- Higher minimum distance from seismic stations (oceanic earthquakes)
- Moderate magnitude and significance

Clusters 3 and 4 represent **low-risk** events characterized by:
- High number of detecting stations (land-based proximity)
- High community-determined intensity (felt on land)
- Earlier historical years

---

# 10. References
**Data License:** Creative Commons Attribution 4.0 International (CC BY 4.0)  
https://creativecommons.org/licenses/by/4.0/